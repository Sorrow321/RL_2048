{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import env.game as game\n",
    "\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_game = game.Game()\n",
    "ob = env_game.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_progress(rewards_batch, loss_batch, eps, reward_range=[50, 5000]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress.\n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    #threshold = np.percentile(rewards_batch, percentile)\n",
    "    #log.append([mean_reward, threshold])\n",
    "\n",
    "    clear_output(True)\n",
    "    #print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n",
    "    plt.figure(figsize=[12, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rewards_batch, label=f'Average reward. Eps: {eps:.4f}')\n",
    "    #plt.plot(list(zip(*log))[1], label=\"Reward thresholds\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(loss_batch, label=\"Average loss\")\n",
    "    #plt.plot(list(zip(*log))[1], label=\"Reward thresholds\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    #plt.hist(rewards_batch, range=reward_range)\n",
    "    #plt.vlines(\n",
    "    #    [np.percentile(rewards_batch, percentile)],\n",
    "    ##    [0],\n",
    "    #   [100],\n",
    "    #    label=\"percentile\",\n",
    "    #    color=\"red\",\n",
    "    #)\n",
    "    #plt.legend()\n",
    "    #plt.grid()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 21])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define the function to encode the state of the 2048 game board\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def encode_board(board):\n",
    "    board = board.flatten()\n",
    "    max_power = 20  # Maximum power of 2 that can be represented\n",
    "    num_classes = max_power + 1  # Including 0, but we'll use zero vectors for empty tiles\n",
    "\n",
    "    # Compute the log2 of the board values where board > 0, set the rest to 0\n",
    "    powers = np.zeros_like(board, dtype=int)\n",
    "    non_zero_indices = board > 0\n",
    "    powers[non_zero_indices] = np.log2(board[non_zero_indices]).astype(int)\n",
    "\n",
    "    # One-hot encode the powers array\n",
    "    one_hot_encoded = np.eye(num_classes)[powers]\n",
    "    \n",
    "    # Set the one-hot encoded vectors corresponding to zero tiles to zero\n",
    "    one_hot_encoded[board == 0] = 0\n",
    "    \n",
    "    # Convert to tensor\n",
    "    one_hot_encoded_tensor = torch.tensor(one_hot_encoded, dtype=torch.float32)\n",
    "    \n",
    "    # Reshape to 4x4x21\n",
    "    one_hot_encoded_tensor = one_hot_encoded_tensor.view(4, 4, num_classes)\n",
    "    \n",
    "    return one_hot_encoded_tensor\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "board = np.array([[2, 4, 0, 2],\n",
    "                  [8, 0, 16, 32],\n",
    "                  [2, 4, 128, 0],\n",
    "                  [256, 0, 1024, 2048]])\n",
    "\n",
    "encoded_board = encode_board(board)\n",
    "print(encoded_board.shape)  # Should print (16, 21) indicating 16 tiles and 21 one-hot encoded features per tile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have an environment class 'Env' as described\n",
    "class Env:\n",
    "    def __init__(self, env_f):\n",
    "        self.env = env_f\n",
    "        \n",
    "    def action(self, a):\n",
    "        next_ob, r, term = self.env.action(a)\n",
    "        next_ob = np.array(next_ob)\n",
    "        encoded_next_ob = encode_board(next_ob)\n",
    "        return (encoded_next_ob.permute(2, 0, 1).unsqueeze(0), r, term)  # Reshape for CNN\n",
    "\n",
    "    def reset(self):\n",
    "        initial_state = self.env.reset()\n",
    "        initial_state = np.array(initial_state)\n",
    "        encoded_initial_state = encode_board(np.array(initial_state))\n",
    "        return encoded_initial_state.permute(2, 0, 1).unsqueeze(0)  # Reshape for CNN\n",
    "\n",
    "\n",
    "\n",
    "# Define the DQN network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=21, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(21 * 4 * 4, 512)  # After flattening from the conv layers\n",
    "        #self.fc2 = nn.Linear(512, 512)\n",
    "        #self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.fc5 = nn.Linear(256, 64)\n",
    "        self.fc6 = nn.Linear(64, 4)  # Output layer (assuming 4 possible actions)\n",
    "        self.lin = nn.Linear(16, 4)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply the convolutional layers with ReLU activations\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        \n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        # Apply the fully connected layers with ReLU activations\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        #x = F.relu(self.fc3(x))\n",
    "        #x = self.dropout(x)\n",
    "        #x = F.relu(self.fc4(x))\n",
    "        #x = self.dropout(x)\n",
    "        #x = F.relu(self.fc5(x))\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        # Output layer (raw scores for each action)\n",
    "        #x = self.fc6(x)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerDQN(nn.Module):\n",
    "    def __init__(self, num_tiles=21, d_model=64, nhead=4, num_layers=3, dim_feedforward=64):\n",
    "        super(TransformerDQN, self).__init__()\n",
    "\n",
    "        # Positional encoding for the 4x4 grid\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(16, d_model))\n",
    "\n",
    "        # Linear projection of the input one-hot vectors to the model dimension\n",
    "        self.input_proj = nn.Linear(num_tiles, d_model)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            batch_first=True  # Setting batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Fully connected layers after the transformer\n",
    "        self.fc1 = nn.Linear(16 * d_model, 4)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Reshape the input tensor to shape (batch_size, 16, 21)\n",
    "        x = x.reshape(batch_size, 21, -1)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # Apply the linear projection\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        # Add positional encoding\n",
    "        #x = x + self.positional_encoding\n",
    "\n",
    "        # Pass through the transformer encoder (batch_first=True ensures no need for transposing)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Flatten the sequence output\n",
    "        x = x.contiguous().view(batch_size, -1)\n",
    "\n",
    "        # Pass through the fully connected layers\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 0.99\n",
    "EPSILON_END = 0.2\n",
    "EPSILON_DECAY = 200000\n",
    "TARGET_UPDATE = 30\n",
    "MEMORY_SIZE = 10000\n",
    "LR = 1e-4\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.pointer = 0\n",
    "        self.capacity = capacity\n",
    "        self.states = torch.zeros((capacity, 21, 4, 4), device='cpu')\n",
    "        self.actions = torch.zeros((capacity, 1), dtype=torch.int64, device='cpu')\n",
    "        self.next_states = torch.zeros_like(self.states, device='cpu')\n",
    "        self.rewards = torch.zeros((capacity, 1), device='cpu')\n",
    "        self.dones = torch.zeros((capacity, 1), dtype=torch.int64, device='cpu')\n",
    "\n",
    "    def push(self, s, a, r, s_new, done):\n",
    "        tar_idx = self.pointer % self.capacity\n",
    "        self.states[tar_idx] = s.cpu()\n",
    "        self.actions[tar_idx] = a.cpu()\n",
    "        self.rewards[tar_idx] = r.cpu()\n",
    "        self.next_states[tar_idx] = s_new.cpu()\n",
    "        self.dones[tar_idx] = done\n",
    "        self.pointer += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        perm = torch.randperm(len(self))\n",
    "        tar_idxes = perm[:batch_size]\n",
    "        return (\n",
    "            self.states[tar_idxes].cuda(),\n",
    "            self.actions[tar_idxes].cuda(),\n",
    "            self.rewards[tar_idxes].cuda(),\n",
    "            self.next_states[tar_idxes].cuda(),\n",
    "            self.dones[tar_idxes].cuda(),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(self.pointer, self.capacity)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize environment, networks, optimizer, and memory\n",
    "env = Env(env_game) #gym.make(\"CartPole-v1\")\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "#policy_net = QNetwork(4*4*21, 4, 42).to(device)\n",
    "#target_net = QNetwork(4*4*21, 4, 42).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, weight_decay=0)\n",
    "memory = ExperienceReplay(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(local_model, target_model, tau):\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    steps_done += 1\n",
    "    if sample > epsilon:\n",
    "        with torch.no_grad(), torch.amp.autocast(device_type='cuda'):\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(4)]], dtype=torch.long, device=device)\n",
    "\n",
    "def optimize_model():\n",
    "    global dt0\n",
    "    global dt1\n",
    "\n",
    "    \n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return 0\n",
    "    t0 = time.time()\n",
    "    state_batch, action_batch, reward_batch, next_state_batch, done_batch = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        Q_s = policy_net(state_batch).gather(1, action_batch)\n",
    "        with torch.no_grad():\n",
    "            # double dqn\n",
    "            best_actions = policy_net(next_state_batch).max(1)[1].unsqueeze(1) \n",
    "            Q_next = target_net(next_state_batch).gather(1, best_actions) * (1-done_batch)\n",
    "            #Q_next = target_net(next_state_batch).max(1)[0].unsqueeze(1).detach()\n",
    "    \n",
    "        target = reward_batch + GAMMA * Q_next\n",
    "        loss = F.mse_loss(Q_s, target)\n",
    "        dt0 += time.time() - t0\n",
    "    \n",
    "        t0 = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        dt1 += time.time() - t0\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = EPSILON_START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON_END = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_UPDATE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[310], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m state_old \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m     41\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m---> 42\u001b[0m loss \u001b[38;5;241m=\u001b[39m optimize_model()\n\u001b[1;32m     43\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[0;32mIn[306], line 24\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m state_batch, action_batch, reward_batch, next_state_batch, done_batch \u001b[38;5;241m=\u001b[39m memory\u001b[38;5;241m.\u001b[39msample(BATCH_SIZE)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 24\u001b[0m     Q_s \u001b[38;5;241m=\u001b[39m policy_net(state_batch)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, action_batch)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;66;03m# double dqn\u001b[39;00m\n\u001b[1;32m     27\u001b[0m         best_actions \u001b[38;5;241m=\u001b[39m policy_net(next_state_batch)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[303], line 49\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4(x))\n\u001b[1;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv5(x))\n\u001b[0;32m---> 49\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv6(x))\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Flatten the tensor for the fully connected layers\u001b[39;00m\n\u001b[1;32m     52\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "num_episodes = 2000000\n",
    "episode_durations = []\n",
    "rewards = []\n",
    "mean_rewards = []\n",
    "losses = []\n",
    "mean_losses = []\n",
    "\n",
    "dt0 = 0\n",
    "dt1 = 0\n",
    "for i_episode in range(num_episodes):\n",
    "    state = torch.tensor(env.reset(), dtype=torch.float32, device=device)\n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    if (i_episode + 1) % 200 == 0:\n",
    "        epsilon *= 2\n",
    "    \n",
    "    for t in count():\n",
    "        #print(state.shape)\n",
    "        epsilon = max(epsilon * 0.99995, EPSILON_END)\n",
    "        #action = select_action(state, epsilon if t > 180 else 0.03)\n",
    "        action = select_action(state, epsilon)\n",
    "        \n",
    "        next_ob, reward, done = env.action(action.item())\n",
    "        if reward > 0:\n",
    "            reward = 1\n",
    "        elif reward == 0:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = -1\n",
    "        if action == 0 or action == 3:\n",
    "            reward -= 0.1\n",
    "        total_reward += reward\n",
    "\n",
    "        reward = torch.tensor([reward], dtype=torch.float32, device=device)\n",
    "        next_state = torch.tensor(next_ob, dtype=torch.float32, device=device) if not done else None\n",
    "\n",
    "        \n",
    "        memory.push(state, action, reward, next_state if not done else state, done)\n",
    "        state_old = state\n",
    "        state = next_state\n",
    "        loss = optimize_model()\n",
    "        total_loss += loss\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            rewards.append(total_reward)\n",
    "            losses.append(total_loss / t)\n",
    "            mean_losses.append(np.mean(losses[-10:]))\n",
    "            mean_rewards.append(np.mean(rewards[-10:]))\n",
    "            break\n",
    "\n",
    "\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    if i_episode % 10 == 0:\n",
    "        show_progress(mean_rewards[-500:], mean_losses[-500:], epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   120,   150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_board(encoded_board):\n",
    "    # encoded_board shape: [batch_size, 21, 4, 4] or [21, 4, 4] if not batched\n",
    "    # First, ensure that the encoded_board is in the correct shape\n",
    "    if encoded_board.dim() == 4:\n",
    "        # If the input is batched, remove the batch dimension\n",
    "        encoded_board = encoded_board.squeeze(0)\n",
    "    \n",
    "    # Permute the encoded board to shape [4, 4, 21] for easier decoding\n",
    "    encoded_board = encoded_board.permute(1, 2, 0)  # Now shape is [4, 4, 21]\n",
    "    \n",
    "    # Get the indices of the maximum values along the last dimension (the one-hot vector dimension)\n",
    "    powers = torch.argmax(encoded_board, dim=-1)\n",
    "    \n",
    "    # Convert the powers back to the corresponding values in the 2048 game board\n",
    "    decoded_board = torch.pow(2, powers).cpu().numpy()\n",
    "    \n",
    "    # Replace 2^0 (which is 1) with 0 to represent empty tiles\n",
    "    decoded_board[decoded_board == 1] = 0\n",
    "    \n",
    "    return decoded_board\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(21, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc4): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc5): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc6): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAMS = 0\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:\n",
      "[[0 0 0 0]\n",
      " [0 0 2 0]\n",
      " [0 0 0 0]\n",
      " [2 0 0 0]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[0 0 0 0]\n",
      " [0 0 0 2]\n",
      " [0 0 0 0]\n",
      " [0 2 0 2]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[0 0 0 0]\n",
      " [0 0 0 2]\n",
      " [0 0 0 0]\n",
      " [0 2 0 2]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[0 0 0 0]\n",
      " [0 0 0 2]\n",
      " [0 0 4 0]\n",
      " [0 0 0 4]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[0 0 0 0]\n",
      " [0 0 0 2]\n",
      " [0 0 4 0]\n",
      " [0 0 0 4]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[0 0 0 0]\n",
      " [0 0 0 2]\n",
      " [0 0 2 4]\n",
      " [0 0 0 4]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[0 0 0 0]\n",
      " [0 0 0 2]\n",
      " [0 0 2 4]\n",
      " [0 0 0 4]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[0 2 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 2]\n",
      " [0 0 2 8]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[0 2 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 2]\n",
      " [0 0 2 8]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[0 0 0 2]\n",
      " [0 0 0 2]\n",
      " [0 0 0 2]\n",
      " [0 0 2 8]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[0 0 0 2]\n",
      " [0 0 0 2]\n",
      " [0 0 0 2]\n",
      " [0 0 2 8]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[0 0 0 2]\n",
      " [0 0 0 2]\n",
      " [0 0 0 4]\n",
      " [0 0 2 8]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[0 0 0 2]\n",
      " [0 0 0 2]\n",
      " [0 0 0 4]\n",
      " [0 0 2 8]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[2 0 0 0]\n",
      " [0 0 0 4]\n",
      " [0 0 0 4]\n",
      " [0 0 2 8]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[2 0 0 0]\n",
      " [0 0 0 4]\n",
      " [0 0 0 4]\n",
      " [0 0 2 8]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[0 0 0 2]\n",
      " [0 0 0 4]\n",
      " [0 2 0 4]\n",
      " [0 0 2 8]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[0 0 0 2]\n",
      " [0 0 0 4]\n",
      " [0 2 0 4]\n",
      " [0 0 2 8]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[0 0 0 2]\n",
      " [0 0 0 4]\n",
      " [0 4 2 4]\n",
      " [0 0 2 8]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[0 0 0 2]\n",
      " [0 0 0 4]\n",
      " [0 4 2 4]\n",
      " [0 0 2 8]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[0 0 0 0]\n",
      " [0 0 0 2]\n",
      " [0 0 4 8]\n",
      " [0 4 4 8]]\n",
      "Reward: 12 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[0 0 0 0]\n",
      " [0 0 0 2]\n",
      " [0 0 4 8]\n",
      " [0 4 4 8]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[0 0 0 0]\n",
      " [0 0 2 2]\n",
      " [0 0 4 8]\n",
      " [0 0 8 8]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[0 0 0 0]\n",
      " [0 0 2 2]\n",
      " [0 0 4 8]\n",
      " [0 0 8 8]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  2  0]\n",
      " [ 0  0  0  4]\n",
      " [ 0  0  4  8]\n",
      " [ 0  0  0 16]]\n",
      "Reward: 20 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  2  0]\n",
      " [ 0  0  0  4]\n",
      " [ 0  0  4  8]\n",
      " [ 0  0  0 16]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 2  0  0  2]\n",
      " [ 0  0  0  4]\n",
      " [ 0  0  4  8]\n",
      " [ 0  0  0 16]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 2  0  0  2]\n",
      " [ 0  0  0  4]\n",
      " [ 0  0  4  8]\n",
      " [ 0  0  0 16]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  4]\n",
      " [ 0  0  0  4]\n",
      " [ 4  0  4  8]\n",
      " [ 0  0  0 16]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  4]\n",
      " [ 0  0  0  4]\n",
      " [ 4  0  4  8]\n",
      " [ 0  0  0 16]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  2  4]\n",
      " [ 0  0  0  4]\n",
      " [ 0  0  8  8]\n",
      " [ 0  0  0 16]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  2  4]\n",
      " [ 0  0  0  4]\n",
      " [ 0  0  8  8]\n",
      " [ 0  0  0 16]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  2  4]\n",
      " [ 0  0  0  4]\n",
      " [ 0  0  2 16]\n",
      " [ 0  0  0 16]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  2  4]\n",
      " [ 0  0  0  4]\n",
      " [ 0  0  2 16]\n",
      " [ 0  0  0 16]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  0  4]\n",
      " [ 0  2  0  8]\n",
      " [ 0  0  4 32]]\n",
      "Reward: 44 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  0  4]\n",
      " [ 0  2  0  8]\n",
      " [ 0  0  4 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  0  4]\n",
      " [ 4  0  2  8]\n",
      " [ 0  0  4 32]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  0  4]\n",
      " [ 4  0  2  8]\n",
      " [ 0  0  4 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  2  4]\n",
      " [ 0  4  2  8]\n",
      " [ 0  0  4 32]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  2  4]\n",
      " [ 0  4  2  8]\n",
      " [ 0  0  4 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  0  4]\n",
      " [ 0  2  4  8]\n",
      " [ 0  4  4 32]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  0  4]\n",
      " [ 0  2  4  8]\n",
      " [ 0  4  4 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 4  0  0  4]\n",
      " [ 0  2  4  8]\n",
      " [ 0  0  8 32]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 4  0  0  4]\n",
      " [ 0  2  4  8]\n",
      " [ 0  0  8 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 2  0  0  0]\n",
      " [ 0  0  0  8]\n",
      " [ 0  2  4  8]\n",
      " [ 0  0  8 32]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 2  0  0  0]\n",
      " [ 0  0  0  8]\n",
      " [ 0  2  4  8]\n",
      " [ 0  0  8 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  2]\n",
      " [ 2  0  0  8]\n",
      " [ 0  2  4  8]\n",
      " [ 0  0  8 32]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  2]\n",
      " [ 2  0  0  8]\n",
      " [ 0  2  4  8]\n",
      " [ 0  0  8 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  2]\n",
      " [ 0  0  2  8]\n",
      " [ 0  2  4  8]\n",
      " [ 0  2  8 32]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  2]\n",
      " [ 0  0  2  8]\n",
      " [ 0  2  4  8]\n",
      " [ 0  2  8 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  2  2]\n",
      " [ 0  2  4 16]\n",
      " [ 0  4  8 32]]\n",
      "Reward: 20 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  2  2]\n",
      " [ 0  2  4 16]\n",
      " [ 0  4  8 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  0  4]\n",
      " [ 0  2  4 16]\n",
      " [ 2  4  8 32]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  0  4]\n",
      " [ 0  2  4 16]\n",
      " [ 2  4  8 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  2  0]\n",
      " [ 4  0  0  0]\n",
      " [ 2  4 16  0]\n",
      " [ 2  4  8 32]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  2  0]\n",
      " [ 4  0  0  0]\n",
      " [ 2  4 16  0]\n",
      " [ 2  4  8 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  2]\n",
      " [ 0  0  0  4]\n",
      " [ 2  2  4 16]\n",
      " [ 2  4  8 32]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  2]\n",
      " [ 0  0  0  4]\n",
      " [ 2  2  4 16]\n",
      " [ 2  4  8 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  2]\n",
      " [ 0  2  0  4]\n",
      " [ 0  4  4 16]\n",
      " [ 2  4  8 32]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  2]\n",
      " [ 0  2  0  4]\n",
      " [ 0  4  4 16]\n",
      " [ 2  4  8 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  2  0  2]\n",
      " [ 0  0  2  4]\n",
      " [ 0  0  8 16]\n",
      " [ 2  4  8 32]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  2  0  2]\n",
      " [ 0  0  2  4]\n",
      " [ 0  0  8 16]\n",
      " [ 2  4  8 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  4]\n",
      " [ 0  0  2  4]\n",
      " [ 0  2  8 16]\n",
      " [ 2  4  8 32]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  4]\n",
      " [ 0  0  2  4]\n",
      " [ 0  2  8 16]\n",
      " [ 2  4  8 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 2  0  0  8]\n",
      " [ 0  2  2 16]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 24 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 2  0  0  8]\n",
      " [ 0  2  2 16]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  2  8]\n",
      " [ 0  4  4 16]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  2  8]\n",
      " [ 0  4  4 16]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  2  0  0]\n",
      " [ 0  0  2  8]\n",
      " [ 0  0  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  2  0  0]\n",
      " [ 0  0  2  8]\n",
      " [ 0  0  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 2  0  0  2]\n",
      " [ 0  0  2  8]\n",
      " [ 0  0  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 2  0  0  2]\n",
      " [ 0  0  2  8]\n",
      " [ 0  0  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  2  0  4]\n",
      " [ 0  0  2  8]\n",
      " [ 0  0  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  2  0  4]\n",
      " [ 0  0  2  8]\n",
      " [ 0  0  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  2  4]\n",
      " [ 2  0  2  8]\n",
      " [ 0  0  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  2  4]\n",
      " [ 2  0  2  8]\n",
      " [ 0  0  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  2  4]\n",
      " [ 0  0  4  8]\n",
      " [ 2  0  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  2  4]\n",
      " [ 0  0  4  8]\n",
      " [ 2  0  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  2  4]\n",
      " [ 2  0  4  8]\n",
      " [ 0  2  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  2  4]\n",
      " [ 2  0  4  8]\n",
      " [ 0  2  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 2  0  2  4]\n",
      " [ 0  2  4  8]\n",
      " [ 0  2  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 2  0  2  4]\n",
      " [ 0  2  4  8]\n",
      " [ 0  2  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  4  4]\n",
      " [ 2  2  4  8]\n",
      " [ 0  2  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  4  4]\n",
      " [ 2  2  4  8]\n",
      " [ 0  2  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  8]\n",
      " [ 4  4  4  8]\n",
      " [ 0  2  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 12 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  8]\n",
      " [ 4  4  4  8]\n",
      " [ 0  2  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  2  8]\n",
      " [ 0  4  8  8]\n",
      " [ 0  2  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  2  8]\n",
      " [ 0  4  8  8]\n",
      " [ 0  2  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  2  8]\n",
      " [ 2  0  4 16]\n",
      " [ 0  2  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  2  8]\n",
      " [ 2  0  4 16]\n",
      " [ 0  2  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  4  2  8]\n",
      " [ 0  2  4 16]\n",
      " [ 0  2  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  4  2  8]\n",
      " [ 0  2  4 16]\n",
      " [ 0  2  8 16]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 2  0  2  0]\n",
      " [ 0  4  4  8]\n",
      " [ 0  4  8 32]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 36 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 2  0  2  0]\n",
      " [ 0  4  4  8]\n",
      " [ 0  4  8 32]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  2  4]\n",
      " [ 0  0  8  8]\n",
      " [ 0  4  8 32]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 12 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  2  4]\n",
      " [ 0  0  8  8]\n",
      " [ 0  4  8 32]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  2  2  4]\n",
      " [ 0  0  0 16]\n",
      " [ 0  4  8 32]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  2  2  4]\n",
      " [ 0  0  0 16]\n",
      " [ 0  4  8 32]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  4  4]\n",
      " [ 0  0  0 16]\n",
      " [ 2  4  8 32]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  4  4]\n",
      " [ 0  0  0 16]\n",
      " [ 2  4  8 32]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  8]\n",
      " [ 0  0  2 16]\n",
      " [ 2  4  8 32]\n",
      " [ 2  4 16 32]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  8]\n",
      " [ 0  0  2 16]\n",
      " [ 2  4  8 32]\n",
      " [ 2  4 16 32]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  2  8]\n",
      " [ 0  2  8 16]\n",
      " [ 4  8 16 64]]\n",
      "Reward: 76 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  2  8]\n",
      " [ 0  2  8 16]\n",
      " [ 4  8 16 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 2  8  0  2]\n",
      " [ 2  8 16  0]\n",
      " [ 4  8 16 64]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 2  8  0  2]\n",
      " [ 2  8 16  0]\n",
      " [ 4  8 16 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  2  8  2]\n",
      " [ 2  2  8 16]\n",
      " [ 4  8 16 64]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 0  2  8  2]\n",
      " [ 2  2  8 16]\n",
      " [ 4  8 16 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 2  2  8  2]\n",
      " [ 0  4  8 16]\n",
      " [ 4  8 16 64]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 2  2  8  2]\n",
      " [ 0  4  8 16]\n",
      " [ 4  8 16 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 2  4  8  2]\n",
      " [ 0  4  8 16]\n",
      " [ 4  8 16 64]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 2  4  8  2]\n",
      " [ 0  4  8 16]\n",
      " [ 4  8 16 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 2  0  0  0]\n",
      " [ 0  0  0  2]\n",
      " [ 2  8 16 16]\n",
      " [ 4  8 16 64]]\n",
      "Reward: 24 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 2  0  0  0]\n",
      " [ 0  0  0  2]\n",
      " [ 2  8 16 16]\n",
      " [ 4  8 16 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 2  0  0  2]\n",
      " [ 0  0  0  2]\n",
      " [ 0  2  8 32]\n",
      " [ 4  8 16 64]]\n",
      "Reward: 32 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 2  0  0  2]\n",
      " [ 0  0  0  2]\n",
      " [ 0  2  8 32]\n",
      " [ 4  8 16 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  4]\n",
      " [ 0  0  2  2]\n",
      " [ 0  2  8 32]\n",
      " [ 4  8 16 64]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  4]\n",
      " [ 0  0  2  2]\n",
      " [ 0  2  8 32]\n",
      " [ 4  8 16 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  4]\n",
      " [ 0  0  2  4]\n",
      " [ 0  2  8 32]\n",
      " [ 4  8 16 64]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  4]\n",
      " [ 0  0  2  4]\n",
      " [ 0  2  8 32]\n",
      " [ 4  8 16 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  4  2  8]\n",
      " [ 0  2  8 32]\n",
      " [ 4  8 16 64]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 0  4  2  8]\n",
      " [ 0  2  8 32]\n",
      " [ 4  8 16 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 4  2  8  0]\n",
      " [ 2  8 32  2]\n",
      " [ 4  8 16 64]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 4  2  8  0]\n",
      " [ 2  8 32  2]\n",
      " [ 4  8 16 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 2  4  2  8]\n",
      " [ 2  8 32  2]\n",
      " [ 4  8 16 64]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 2  4  2  8]\n",
      " [ 2  8 32  2]\n",
      " [ 4  8 16 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 0  2  2  8]\n",
      " [ 4  4 32  2]\n",
      " [ 4 16 16 64]]\n",
      "Reward: 20 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 0  2  2  8]\n",
      " [ 4  4 32  2]\n",
      " [ 4 16 16 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  0]\n",
      " [ 2  0  4  8]\n",
      " [ 0  8 32  2]\n",
      " [ 0  4 32 64]]\n",
      "Reward: 44 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  0]\n",
      " [ 2  0  4  8]\n",
      " [ 0  8 32  2]\n",
      " [ 0  4 32 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  2  0  0]\n",
      " [ 0  2  4  8]\n",
      " [ 0  8 32  2]\n",
      " [ 0  4 32 64]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  2  0  0]\n",
      " [ 0  2  4  8]\n",
      " [ 0  8 32  2]\n",
      " [ 0  4 32 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 2  0  0  2]\n",
      " [ 0  2  4  8]\n",
      " [ 0  8 32  2]\n",
      " [ 0  4 32 64]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 2  0  0  2]\n",
      " [ 0  2  4  8]\n",
      " [ 0  8 32  2]\n",
      " [ 0  4 32 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  4]\n",
      " [ 2  2  4  8]\n",
      " [ 0  8 32  2]\n",
      " [ 0  4 32 64]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  4]\n",
      " [ 2  2  4  8]\n",
      " [ 0  8 32  2]\n",
      " [ 0  4 32 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  2  0  4]\n",
      " [ 0  4  4  8]\n",
      " [ 0  8 32  2]\n",
      " [ 0  4 32 64]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  2  0  4]\n",
      " [ 0  4  4  8]\n",
      " [ 0  8 32  2]\n",
      " [ 0  4 32 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  2  4]\n",
      " [ 2  0  8  8]\n",
      " [ 0  8 32  2]\n",
      " [ 0  4 32 64]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  2  4]\n",
      " [ 2  0  8  8]\n",
      " [ 0  8 32  2]\n",
      " [ 0  4 32 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  2  4]\n",
      " [ 4  0  2 16]\n",
      " [ 0  8 32  2]\n",
      " [ 0  4 32 64]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  2  4]\n",
      " [ 4  0  2 16]\n",
      " [ 0  8 32  2]\n",
      " [ 0  4 32 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 2  0  2  4]\n",
      " [ 0  4  2 16]\n",
      " [ 0  8 32  2]\n",
      " [ 0  4 32 64]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 2  0  2  4]\n",
      " [ 0  4  2 16]\n",
      " [ 0  8 32  2]\n",
      " [ 0  4 32 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  4  4]\n",
      " [ 0  4  2 16]\n",
      " [ 0  8 32  2]\n",
      " [ 2  4 32 64]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  4  4]\n",
      " [ 0  4  2 16]\n",
      " [ 0  8 32  2]\n",
      " [ 2  4 32 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  8]\n",
      " [ 0  4  2 16]\n",
      " [ 2  8 32  2]\n",
      " [ 2  4 32 64]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  8]\n",
      " [ 0  4  2 16]\n",
      " [ 2  8 32  2]\n",
      " [ 2  4 32 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 0  0  0  8]\n",
      " [ 0  4  0 16]\n",
      " [ 2  8  2  2]\n",
      " [ 4  4 64 64]]\n",
      "Reward: 68 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 0  0  0  8]\n",
      " [ 0  4  0 16]\n",
      " [ 2  8  2  2]\n",
      " [ 4  4 64 64]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   0   8]\n",
      " [  0   0   4  16]\n",
      " [  0   2   8   4]\n",
      " [  0   4   8 128]]\n",
      "Reward: 140 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   0   8]\n",
      " [  0   0   4  16]\n",
      " [  0   2   8   4]\n",
      " [  0   4   8 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2   8]\n",
      " [  0   2   4  16]\n",
      " [  0   2   8   4]\n",
      " [  0   4   8 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2   8]\n",
      " [  0   2   4  16]\n",
      " [  0   2   8   4]\n",
      " [  0   4   8 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   0   8]\n",
      " [  0   0   2  16]\n",
      " [  0   4   4   4]\n",
      " [  0   4  16 128]]\n",
      "Reward: 20 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   0   8]\n",
      " [  0   0   2  16]\n",
      " [  0   4   4   4]\n",
      " [  0   4  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2   8]\n",
      " [  0   0   2  16]\n",
      " [  0   0   4   8]\n",
      " [  4   4  16 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2   8]\n",
      " [  0   0   2  16]\n",
      " [  0   0   4   8]\n",
      " [  4   4  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2   8]\n",
      " [  2   0   2  16]\n",
      " [  0   0   4   8]\n",
      " [  0   8  16 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2   8]\n",
      " [  2   0   2  16]\n",
      " [  0   0   4   8]\n",
      " [  0   8  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2   8]\n",
      " [  0   0   4  16]\n",
      " [  0   0   4   8]\n",
      " [  2   8  16 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2   8]\n",
      " [  0   0   4  16]\n",
      " [  0   0   4   8]\n",
      " [  2   8  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   0   8]\n",
      " [  0   0   2  16]\n",
      " [  0   0   8   8]\n",
      " [  2   8  16 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   0   8]\n",
      " [  0   0   2  16]\n",
      " [  0   0   8   8]\n",
      " [  2   8  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2   8]\n",
      " [  0   0   2  16]\n",
      " [  0   0   2  16]\n",
      " [  2   8  16 128]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2   8]\n",
      " [  0   0   2  16]\n",
      " [  0   0   2  16]\n",
      " [  2   8  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   0   0]\n",
      " [  0   2   2   8]\n",
      " [  0   0   4  32]\n",
      " [  2   8  16 128]]\n",
      "Reward: 36 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   0   0]\n",
      " [  0   2   2   8]\n",
      " [  0   0   4  32]\n",
      " [  2   8  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   0   0]\n",
      " [  0   0   4   8]\n",
      " [  2   0   4  32]\n",
      " [  2   8  16 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   0   0]\n",
      " [  0   0   4   8]\n",
      " [  2   0   4  32]\n",
      " [  2   8  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   0   0]\n",
      " [  0   2   4   8]\n",
      " [  0   2   4  32]\n",
      " [  2   8  16 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   0   0]\n",
      " [  0   2   4   8]\n",
      " [  0   2   4  32]\n",
      " [  2   8  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   0   0]\n",
      " [  0   0   2   8]\n",
      " [  0   4   8  32]\n",
      " [  2   8  16 128]]\n",
      "Reward: 12 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   0   0]\n",
      " [  0   0   2   8]\n",
      " [  0   4   8  32]\n",
      " [  2   8  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   0   0]\n",
      " [  2   8   0   0]\n",
      " [  4   8  32   2]\n",
      " [  2   8  16 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   0   0]\n",
      " [  2   8   0   0]\n",
      " [  4   8  32   2]\n",
      " [  2   8  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   0   0]\n",
      " [  0   2   2   8]\n",
      " [  4   8  32   2]\n",
      " [  2   8  16 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   0   0]\n",
      " [  0   2   2   8]\n",
      " [  4   8  32   2]\n",
      " [  2   8  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   0   0]\n",
      " [  0   0   4   8]\n",
      " [  4   8  32   2]\n",
      " [  2   8  16 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   0   0]\n",
      " [  0   0   4   8]\n",
      " [  4   8  32   2]\n",
      " [  2   8  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   0   2]\n",
      " [  2   0   4   8]\n",
      " [  4   8  32   2]\n",
      " [  2   8  16 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   0   2]\n",
      " [  2   0   4   8]\n",
      " [  4   8  32   2]\n",
      " [  2   8  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   0   2]\n",
      " [  0   2   4   8]\n",
      " [  4   8  32   2]\n",
      " [  2   8  16 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   0   2]\n",
      " [  0   2   4   8]\n",
      " [  4   8  32   2]\n",
      " [  2   8  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2   4]\n",
      " [  0   2   4   8]\n",
      " [  4   8  32   2]\n",
      " [  2   8  16 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2   4]\n",
      " [  0   2   4   8]\n",
      " [  4   8  32   2]\n",
      " [  2   8  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   2   4]\n",
      " [  0   0   4   8]\n",
      " [  4   2  32   2]\n",
      " [  2  16  16 128]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   2   4]\n",
      " [  0   0   4   8]\n",
      " [  4   2  32   2]\n",
      " [  2  16  16 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   4   4]\n",
      " [  0   2   4   8]\n",
      " [  4   2  32   2]\n",
      " [  0   2  32 128]]\n",
      "Reward: 36 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   4   4]\n",
      " [  0   2   4   8]\n",
      " [  4   2  32   2]\n",
      " [  0   2  32 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   0   8]\n",
      " [  0   2   4   8]\n",
      " [  4   2  32   2]\n",
      " [  0   2  32 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   0   8]\n",
      " [  0   2   4   8]\n",
      " [  4   2  32   2]\n",
      " [  0   2  32 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   2   8]\n",
      " [  0   2   4   8]\n",
      " [  4   2  32   2]\n",
      " [  0   2  32 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   2   8]\n",
      " [  0   2   4   8]\n",
      " [  4   2  32   2]\n",
      " [  0   2  32 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   4   8]\n",
      " [  0   2   4   8]\n",
      " [  4   2  32   2]\n",
      " [  2   2  32 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   4   8]\n",
      " [  0   2   4   8]\n",
      " [  4   2  32   2]\n",
      " [  2   2  32 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   4   8]\n",
      " [  0   2   4   8]\n",
      " [  4   2  32   2]\n",
      " [  2   4  32 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   4   8]\n",
      " [  0   2   4   8]\n",
      " [  4   2  32   2]\n",
      " [  2   4  32 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   0   0]\n",
      " [  0   0   4  16]\n",
      " [  4   4   8   2]\n",
      " [  2   4  64 128]]\n",
      "Reward: 92 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   0   0]\n",
      " [  0   0   4  16]\n",
      " [  4   4   8   2]\n",
      " [  2   4  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2   2]\n",
      " [  0   0   4  16]\n",
      " [  0   8   8   2]\n",
      " [  2   4  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2   2]\n",
      " [  0   0   4  16]\n",
      " [  0   8   8   2]\n",
      " [  2   4  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   0   4]\n",
      " [  0   2   4  16]\n",
      " [  0   0  16   2]\n",
      " [  2   4  64 128]]\n",
      "Reward: 20 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   0   4]\n",
      " [  0   2   4  16]\n",
      " [  0   0  16   2]\n",
      " [  2   4  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   0   4]\n",
      " [  0   0   4  16]\n",
      " [  0   2  16   2]\n",
      " [  2   4  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   0   4]\n",
      " [  0   0   4  16]\n",
      " [  0   2  16   2]\n",
      " [  2   4  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   2   4]\n",
      " [  0   0   4  16]\n",
      " [  0   2  16   2]\n",
      " [  2   4  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   2   4]\n",
      " [  0   0   4  16]\n",
      " [  0   2  16   2]\n",
      " [  2   4  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   4   4]\n",
      " [  0   0   4  16]\n",
      " [  2   2  16   2]\n",
      " [  2   4  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   4   4]\n",
      " [  0   0   4  16]\n",
      " [  2   2  16   2]\n",
      " [  2   4  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   0   8]\n",
      " [  0   0   4  16]\n",
      " [  0   4  16   2]\n",
      " [  2   4  64 128]]\n",
      "Reward: 12 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   0   8]\n",
      " [  0   0   4  16]\n",
      " [  0   4  16   2]\n",
      " [  2   4  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  4   0   2   8]\n",
      " [  0   0   4  16]\n",
      " [  0   4  16   2]\n",
      " [  2   4  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  4   0   2   8]\n",
      " [  0   0   4  16]\n",
      " [  0   4  16   2]\n",
      " [  2   4  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   2   8]\n",
      " [  0   0   4  16]\n",
      " [  0   4  16   2]\n",
      " [  2   4  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   2   8]\n",
      " [  0   0   4  16]\n",
      " [  0   4  16   2]\n",
      " [  2   4  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   2   8]\n",
      " [  0   0   4  16]\n",
      " [  0   4  16   2]\n",
      " [  4   8  64 128]]\n",
      "Reward: 12 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   2   8]\n",
      " [  0   0   4  16]\n",
      " [  0   4  16   2]\n",
      " [  4   8  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   4   8]\n",
      " [  2   0   4  16]\n",
      " [  0   4  16   2]\n",
      " [  4   8  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   4   8]\n",
      " [  2   0   4  16]\n",
      " [  0   4  16   2]\n",
      " [  4   8  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   4   8]\n",
      " [  2   2   4  16]\n",
      " [  0   4  16   2]\n",
      " [  4   8  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   4   8]\n",
      " [  2   2   4  16]\n",
      " [  0   4  16   2]\n",
      " [  4   8  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   4   8]\n",
      " [  0   4   4  16]\n",
      " [  0   4  16   2]\n",
      " [  4   8  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   4   8]\n",
      " [  0   4   4  16]\n",
      " [  0   4  16   2]\n",
      " [  4   8  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   4   8]\n",
      " [  0   2   8  16]\n",
      " [  0   4  16   2]\n",
      " [  4   8  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   4   8]\n",
      " [  0   2   8  16]\n",
      " [  0   4  16   2]\n",
      " [  4   8  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   4   8]\n",
      " [  0   4   8  16]\n",
      " [  0   4  16   2]\n",
      " [  4   8  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   4   8]\n",
      " [  0   4   8  16]\n",
      " [  0   4  16   2]\n",
      " [  4   8  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   4   8]\n",
      " [  0   2   8  16]\n",
      " [  2   8  16   2]\n",
      " [  4   8  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   4   8]\n",
      " [  0   2   8  16]\n",
      " [  2   8  16   2]\n",
      " [  4   8  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   4   8]\n",
      " [  0   0   8  16]\n",
      " [  2   2  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   4   8]\n",
      " [  0   0   8  16]\n",
      " [  2   2  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   2   4   8]\n",
      " [  0   0   8  16]\n",
      " [  0   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   2   4   8]\n",
      " [  0   0   8  16]\n",
      " [  0   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   4   8]\n",
      " [  0   0   8  16]\n",
      " [  0   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   4   8]\n",
      " [  0   0   8  16]\n",
      " [  0   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   8   8]\n",
      " [  0   2   8  16]\n",
      " [  0   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   8   8]\n",
      " [  0   2   8  16]\n",
      " [  0   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2  16]\n",
      " [  0   2   8  16]\n",
      " [  2   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2  16]\n",
      " [  0   2   8  16]\n",
      " [  2   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2   2]\n",
      " [  0   2   8  32]\n",
      " [  2   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 32 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2   2]\n",
      " [  0   2   8  32]\n",
      " [  2   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   0   4]\n",
      " [  0   2   8  32]\n",
      " [  2   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   0   4]\n",
      " [  0   2   8  32]\n",
      " [  2   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   2   4]\n",
      " [  0   2   8  32]\n",
      " [  2   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   2   4]\n",
      " [  0   2   8  32]\n",
      " [  2   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   4   4]\n",
      " [  0   2   8  32]\n",
      " [  2   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   4   4]\n",
      " [  0   2   8  32]\n",
      " [  2   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   2   8]\n",
      " [  0   2   8  32]\n",
      " [  2   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   2   8]\n",
      " [  0   2   8  32]\n",
      " [  2   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   4   8]\n",
      " [  2   2   8  32]\n",
      " [  2   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   4   8]\n",
      " [  2   2   8  32]\n",
      " [  2   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   4   8]\n",
      " [  0   4   8  32]\n",
      " [  2   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   4   8]\n",
      " [  0   4   8  32]\n",
      " [  2   4  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   4   8]\n",
      " [  2   2   8  32]\n",
      " [  2   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   4   8]\n",
      " [  2   2   8  32]\n",
      " [  2   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   4   8]\n",
      " [  0   4   8  32]\n",
      " [  2   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   4   8]\n",
      " [  0   4   8  32]\n",
      " [  2   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   4   8]\n",
      " [  2   4   8  32]\n",
      " [  2   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   4   8]\n",
      " [  2   4   8  32]\n",
      " [  2   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   2   4   8]\n",
      " [  0   4   8  32]\n",
      " [  4   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   2   4   8]\n",
      " [  0   4   8  32]\n",
      " [  4   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   4   8]\n",
      " [  0   4   8  32]\n",
      " [  4   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   4   8]\n",
      " [  0   4   8  32]\n",
      " [  4   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   8   8]\n",
      " [  2   4   8  32]\n",
      " [  4   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   8   8]\n",
      " [  2   4   8  32]\n",
      " [  4   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   2  16]\n",
      " [  2   4   8  32]\n",
      " [  4   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   2  16]\n",
      " [  2   4   8  32]\n",
      " [  4   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  4   0   4  16]\n",
      " [  2   4   8  32]\n",
      " [  4   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  4   0   4  16]\n",
      " [  2   4   8  32]\n",
      " [  4   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4   8  16]\n",
      " [  2   4   8  32]\n",
      " [  4   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4   8  16]\n",
      " [  2   4   8  32]\n",
      " [  4   8  16   2]\n",
      " [  4  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  4   0   0  16]\n",
      " [  0   8  16  32]\n",
      " [  2   8  16   2]\n",
      " [  8  16  64 128]]\n",
      "Reward: 32 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  4   0   0  16]\n",
      " [  0   8  16  32]\n",
      " [  2   8  16   2]\n",
      " [  8  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   4  16]\n",
      " [  0   8  16  32]\n",
      " [  2   8  16   2]\n",
      " [  8  16  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   4  16]\n",
      " [  0   8  16  32]\n",
      " [  2   8  16   2]\n",
      " [  8  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   4  16]\n",
      " [  4   8  16  32]\n",
      " [  2   8  16   2]\n",
      " [  8  16  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   4  16]\n",
      " [  4   8  16  32]\n",
      " [  2   8  16   2]\n",
      " [  8  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2  16]\n",
      " [  4   2   4  32]\n",
      " [  2  16  32   2]\n",
      " [  8  16  64 128]]\n",
      "Reward: 48 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2  16]\n",
      " [  4   2   4  32]\n",
      " [  2  16  32   2]\n",
      " [  8  16  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   2  16]\n",
      " [  4   0   4  32]\n",
      " [  2   2  32   2]\n",
      " [  8  32  64 128]]\n",
      "Reward: 32 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   2  16]\n",
      " [  4   0   4  32]\n",
      " [  2   2  32   2]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4   4  16]\n",
      " [  0   0   8  32]\n",
      " [  0   4  32   2]\n",
      " [  8  32  64 128]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4   4  16]\n",
      " [  0   0   8  32]\n",
      " [  0   4  32   2]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   8  16]\n",
      " [  2   0   8  32]\n",
      " [  0   4  32   2]\n",
      " [  8  32  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   8  16]\n",
      " [  2   0   8  32]\n",
      " [  0   4  32   2]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   8  16]\n",
      " [  0   2   8  32]\n",
      " [  0   4  32   2]\n",
      " [  8  32  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   8  16]\n",
      " [  0   2   8  32]\n",
      " [  0   4  32   2]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   0  16]\n",
      " [  0   4  16  32]\n",
      " [  0   4  32   2]\n",
      " [  8  32  64 128]]\n",
      "Reward: 20 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   0  16]\n",
      " [  0   4  16  32]\n",
      " [  0   4  32   2]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   2  16]\n",
      " [  0   4  16  32]\n",
      " [  0   4  32   2]\n",
      " [  8  32  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   2  16]\n",
      " [  0   4  16  32]\n",
      " [  0   4  32   2]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   4  16]\n",
      " [  0   4  16  32]\n",
      " [  4   4  32   2]\n",
      " [  8  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   4  16]\n",
      " [  0   4  16  32]\n",
      " [  4   4  32   2]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   4  16]\n",
      " [  0   4  16  32]\n",
      " [  2   8  32   2]\n",
      " [  8  32  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   4  16]\n",
      " [  0   4  16  32]\n",
      " [  2   8  32   2]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  4  16   0   2]\n",
      " [  4  16  32   0]\n",
      " [  2   8  32   2]\n",
      " [  8  32  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  4  16   0   2]\n",
      " [  4  16  32   0]\n",
      " [  2   8  32   2]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4  16   2]\n",
      " [  2   4  16  32]\n",
      " [  2   8  32   2]\n",
      " [  8  32  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4  16   2]\n",
      " [  2   4  16  32]\n",
      " [  2   8  32   2]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   0   2]\n",
      " [  2   8  32  32]\n",
      " [  4   8  32   2]\n",
      " [  8  32  64 128]]\n",
      "Reward: 44 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   0   2]\n",
      " [  2   8  32  32]\n",
      " [  4   8  32   2]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   4   2]\n",
      " [  0   2   8  64]\n",
      " [  4   8  32   2]\n",
      " [  8  32  64 128]]\n",
      "Reward: 64 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   4   2]\n",
      " [  0   2   8  64]\n",
      " [  4   8  32   2]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  4   2   0   0]\n",
      " [  2   8  64   2]\n",
      " [  4   8  32   2]\n",
      " [  8  32  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  4   2   0   0]\n",
      " [  2   8  64   2]\n",
      " [  4   8  32   2]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   4   2]\n",
      " [  2   8  64   2]\n",
      " [  4   8  32   2]\n",
      " [  8  32  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   4   2]\n",
      " [  2   8  64   2]\n",
      " [  4   8  32   2]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4   4   0]\n",
      " [  2   2  64   2]\n",
      " [  4  16  32   4]\n",
      " [  8  32  64 128]]\n",
      "Reward: 20 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4   4   0]\n",
      " [  2   2  64   2]\n",
      " [  4  16  32   4]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2   8]\n",
      " [  0   4  64   2]\n",
      " [  4  16  32   4]\n",
      " [  8  32  64 128]]\n",
      "Reward: 12 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2   8]\n",
      " [  0   4  64   2]\n",
      " [  4  16  32   4]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   8   2   0]\n",
      " [  4  64   2   0]\n",
      " [  4  16  32   4]\n",
      " [  8  32  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   8   2   0]\n",
      " [  4  64   2   0]\n",
      " [  4  16  32   4]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  4   2   8   2]\n",
      " [  0   4  64   2]\n",
      " [  4  16  32   4]\n",
      " [  8  32  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  4   2   8   2]\n",
      " [  0   4  64   2]\n",
      " [  4  16  32   4]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   8   0]\n",
      " [  2   4  64   4]\n",
      " [  8  16  32   4]\n",
      " [  8  32  64 128]]\n",
      "Reward: 12 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   8   0]\n",
      " [  2   4  64   4]\n",
      " [  8  16  32   4]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   2   8]\n",
      " [  2   4  64   4]\n",
      " [  8  16  32   4]\n",
      " [  8  32  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   2   8]\n",
      " [  2   4  64   4]\n",
      " [  8  16  32   4]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   4   8]\n",
      " [  2   4  64   4]\n",
      " [  8  16  32   4]\n",
      " [  8  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   4   8]\n",
      " [  2   4  64   4]\n",
      " [  8  16  32   4]\n",
      " [  8  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   2   4   0]\n",
      " [  0   4  64   8]\n",
      " [  2  16  32   8]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 24 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   2   4   0]\n",
      " [  0   4  64   8]\n",
      " [  2  16  32   8]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   4   4]\n",
      " [  0   4  64   8]\n",
      " [  2  16  32   8]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   4   4]\n",
      " [  0   4  64   8]\n",
      " [  2  16  32   8]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   2   8]\n",
      " [  0   4  64   8]\n",
      " [  2  16  32   8]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   2   8]\n",
      " [  0   4  64   8]\n",
      " [  2  16  32   8]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   4   8]\n",
      " [  0   4  64   8]\n",
      " [  2  16  32   8]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   4   8]\n",
      " [  0   4  64   8]\n",
      " [  2  16  32   8]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  4   2   4   8]\n",
      " [  0   4  64   8]\n",
      " [  2  16  32   8]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  4   2   4   8]\n",
      " [  0   4  64   8]\n",
      " [  2  16  32   8]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   2   4   0]\n",
      " [  4   4  64   8]\n",
      " [  2  16  32  16]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   2   4   0]\n",
      " [  4   4  64   8]\n",
      " [  2  16  32  16]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   4   4]\n",
      " [  0   8  64   8]\n",
      " [  2  16  32  16]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 12 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   4   4]\n",
      " [  0   8  64   8]\n",
      " [  2  16  32  16]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2   8]\n",
      " [  2   8  64   8]\n",
      " [  2  16  32  16]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2   8]\n",
      " [  2   8  64   8]\n",
      " [  2  16  32  16]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2   2]\n",
      " [  0   8  64  16]\n",
      " [  4  16  32  16]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 20 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2   2]\n",
      " [  0   8  64  16]\n",
      " [  4  16  32  16]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   0   4]\n",
      " [  0   8  64  16]\n",
      " [  4  16  32  16]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   0   4]\n",
      " [  0   8  64  16]\n",
      " [  4  16  32  16]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   2   4]\n",
      " [  0   8  64  16]\n",
      " [  4  16  32  16]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   2   4]\n",
      " [  0   8  64  16]\n",
      " [  4  16  32  16]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   4   4]\n",
      " [  0   8  64  16]\n",
      " [  4  16  32  16]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   4   4]\n",
      " [  0   8  64  16]\n",
      " [  4  16  32  16]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2   8]\n",
      " [  2   8  64  16]\n",
      " [  4  16  32  16]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2   8]\n",
      " [  2   8  64  16]\n",
      " [  4  16  32  16]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   2   0]\n",
      " [  2   8  64   8]\n",
      " [  4  16  32  32]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 32 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   2   0]\n",
      " [  2   8  64   8]\n",
      " [  4  16  32  32]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2   4]\n",
      " [  2   8  64   8]\n",
      " [  0   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 68 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2   4]\n",
      " [  2   8  64   8]\n",
      " [  0   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4   2   4]\n",
      " [  0   8  64   8]\n",
      " [  2   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4   2   4]\n",
      " [  0   8  64   8]\n",
      " [  2   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  4   2   4   2]\n",
      " [  8  64   8   0]\n",
      " [  2   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  4   2   4   2]\n",
      " [  8  64   8   0]\n",
      " [  2   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  4   2   4   2]\n",
      " [  4   8  64   8]\n",
      " [  2   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  4   2   4   2]\n",
      " [  4   8  64   8]\n",
      " [  2   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   2   4   2]\n",
      " [  8   8  64   8]\n",
      " [  2   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   2   4   2]\n",
      " [  8   8  64   8]\n",
      " [  2   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   4   2]\n",
      " [  0  16  64   8]\n",
      " [  2   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 20 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   4   2]\n",
      " [  0  16  64   8]\n",
      " [  2   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   2   8   2]\n",
      " [  0  16  64   8]\n",
      " [  2   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   2   8   2]\n",
      " [  0  16  64   8]\n",
      " [  2   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   2]\n",
      " [  0  16  64   8]\n",
      " [  2   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   2]\n",
      " [  0  16  64   8]\n",
      " [  2   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4   8   2]\n",
      " [  2  16  64   8]\n",
      " [  4   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4   8   2]\n",
      " [  2  16  64   8]\n",
      " [  4   4  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4   8   2]\n",
      " [  2  16  64   8]\n",
      " [  2   8  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4   8   2]\n",
      " [  2  16  64   8]\n",
      " [  2   8  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4   8   2]\n",
      " [  2  16  64   8]\n",
      " [  4   8  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4   8   2]\n",
      " [  2  16  64   8]\n",
      " [  4   8  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  4   8   2   2]\n",
      " [  2  16  64   8]\n",
      " [  4   8  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  4   8   2   2]\n",
      " [  2  16  64   8]\n",
      " [  4   8  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  4   8  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  4   8  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   4]\n",
      " [  4  16  64   8]\n",
      " [  4   8  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   4]\n",
      " [  4  16  64   8]\n",
      " [  4   8  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  8   8  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  8   8  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  2  16  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  2  16  16  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  2   2  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 32 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  2   2  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  2   4  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  2   4  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  4   4  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  4   4  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  2   8  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  2   8  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  4   8  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  4   8  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   4]\n",
      " [  4  16  64   8]\n",
      " [  4   8  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   4]\n",
      " [  4  16  64   8]\n",
      " [  4   8  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  8   8  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  8   8  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  2  16  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   4]\n",
      " [  2  16  64   8]\n",
      " [  2  16  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   8   4]\n",
      " [  2   4  64   8]\n",
      " [  4  32  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 36 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   8   4]\n",
      " [  2   4  64   8]\n",
      " [  4  32  32  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   8   4]\n",
      " [  2   4  64   8]\n",
      " [  2   4  64  64]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 64 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   8   4]\n",
      " [  2   4  64   8]\n",
      " [  2   4  64  64]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   2   8   4]\n",
      " [  2   4  64   8]\n",
      " [  0   2   4 128]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 128 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   2   8   4]\n",
      " [  2   4  64   8]\n",
      " [  0   2   4 128]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4   8   4]\n",
      " [  2   4  64   8]\n",
      " [  2   2   4 128]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4   8   4]\n",
      " [  2   4  64   8]\n",
      " [  2   2   4 128]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   4]\n",
      " [  2   4  64   8]\n",
      " [  0   4   4 128]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   4]\n",
      " [  2   4  64   8]\n",
      " [  0   4   4 128]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   4]\n",
      " [  2   4  64   8]\n",
      " [  2   0   8 128]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   4]\n",
      " [  2   4  64   8]\n",
      " [  2   0   8 128]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   4]\n",
      " [  2   4  64   8]\n",
      " [  2   2   8 128]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   4]\n",
      " [  2   4  64   8]\n",
      " [  2   2   8 128]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8   4]\n",
      " [  2   4  64   8]\n",
      " [  2   4   8 128]\n",
      " [ 16  32  64 128]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8   4]\n",
      " [  2   4  64   8]\n",
      " [  2   4   8 128]\n",
      " [ 16  32  64 128]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   8   0]\n",
      " [  2   4  64   4]\n",
      " [  4   8   8   8]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 268 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   8   0]\n",
      " [  2   4  64   4]\n",
      " [  4   8   8   8]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   2   8]\n",
      " [  2   4  64   4]\n",
      " [  0   4   8  16]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   2   8]\n",
      " [  2   4  64   4]\n",
      " [  0   4   8  16]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   4   8]\n",
      " [  2   4  64   4]\n",
      " [  0   4   8  16]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   4   8]\n",
      " [  2   4  64   4]\n",
      " [  0   4   8  16]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  4   2   4   8]\n",
      " [  2   4  64   4]\n",
      " [  0   4   8  16]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  4   2   4   8]\n",
      " [  2   4  64   4]\n",
      " [  0   4   8  16]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  4   0   4   8]\n",
      " [  4   2  64   4]\n",
      " [  2   8   8  16]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  4   0   4   8]\n",
      " [  4   2  64   4]\n",
      " [  2   8   8  16]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   8   8]\n",
      " [  4   2  64   4]\n",
      " [  0   2  16  16]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 24 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   8   8]\n",
      " [  4   2  64   4]\n",
      " [  0   2  16  16]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2  16]\n",
      " [  4   2  64   4]\n",
      " [  2   0   2  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 48 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2  16]\n",
      " [  4   2  64   4]\n",
      " [  2   0   2  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2  16]\n",
      " [  4   2  64   4]\n",
      " [  2   0   4  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2  16]\n",
      " [  4   2  64   4]\n",
      " [  2   0   4  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2  16]\n",
      " [  4   2  64   4]\n",
      " [  2   2   4  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2  16]\n",
      " [  4   2  64   4]\n",
      " [  2   2   4  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   2  16]\n",
      " [  4   2  64   4]\n",
      " [  2   4   4  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   2  16]\n",
      " [  4   2  64   4]\n",
      " [  2   4   4  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   2  16]\n",
      " [  4   2  64   4]\n",
      " [  0   2   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   2  16]\n",
      " [  4   2  64   4]\n",
      " [  0   2   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   4  16]\n",
      " [  4   2  64   4]\n",
      " [  0   2   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   4  16]\n",
      " [  4   2  64   4]\n",
      " [  0   2   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   2   4  16]\n",
      " [  4   2  64   4]\n",
      " [  0   2   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   2   4  16]\n",
      " [  4   2  64   4]\n",
      " [  0   2   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   4  16]\n",
      " [  4   2  64   4]\n",
      " [  0   2   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   4  16]\n",
      " [  4   2  64   4]\n",
      " [  0   2   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   8  16]\n",
      " [  4   2  64   4]\n",
      " [  2   2   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   8  16]\n",
      " [  4   2  64   4]\n",
      " [  2   2   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   8  16]\n",
      " [  4   2  64   4]\n",
      " [  2   4   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   8  16]\n",
      " [  4   2  64   4]\n",
      " [  2   4   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4   8  16]\n",
      " [  4   4  64   4]\n",
      " [  2   4   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4   8  16]\n",
      " [  4   4  64   4]\n",
      " [  2   4   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4   8  16]\n",
      " [  2   8  64   4]\n",
      " [  2   4   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4   8  16]\n",
      " [  2   8  64   4]\n",
      " [  2   4   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4   8  16]\n",
      " [  2   8  64   4]\n",
      " [  4   4   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4   8  16]\n",
      " [  2   8  64   4]\n",
      " [  4   4   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4   8  16]\n",
      " [  2   8  64   4]\n",
      " [  2   8   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4   8  16]\n",
      " [  2   8  64   4]\n",
      " [  2   8   8  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4   8  16]\n",
      " [  2   8  64   4]\n",
      " [  2   2  16  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4   8  16]\n",
      " [  2   8  64   4]\n",
      " [  2   2  16  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8  16]\n",
      " [  2   8  64   4]\n",
      " [  0   4  16  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8  16]\n",
      " [  2   8  64   4]\n",
      " [  0   4  16  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8  16]\n",
      " [  0   8  64   4]\n",
      " [  4   4  16  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8  16]\n",
      " [  0   8  64   4]\n",
      " [  4   4  16  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4   8  16]\n",
      " [  2   8  64   4]\n",
      " [  0   8  16  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4   8  16]\n",
      " [  2   8  64   4]\n",
      " [  0   8  16  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   8  16]\n",
      " [  0   4  64   4]\n",
      " [  4  16  16  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 20 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   8  16]\n",
      " [  0   4  64   4]\n",
      " [  4  16  16  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   2   8  16]\n",
      " [  0   4  64   4]\n",
      " [  0   4  32  32]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 32 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   2   8  16]\n",
      " [  0   4  64   4]\n",
      " [  0   4  32  32]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4   8  16]\n",
      " [  2   4  64   4]\n",
      " [  0   0   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 68 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4   8  16]\n",
      " [  2   4  64   4]\n",
      " [  0   0   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   8  16]\n",
      " [  2   0  64   4]\n",
      " [  2   8   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   8  16]\n",
      " [  2   0  64   4]\n",
      " [  2   8   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   8  16]\n",
      " [  2   2  64   4]\n",
      " [  2   8   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   8  16]\n",
      " [  2   2  64   4]\n",
      " [  2   8   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   8  16]\n",
      " [  0   4  64   4]\n",
      " [  2   8   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   8  16]\n",
      " [  0   4  64   4]\n",
      " [  2   8   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   8  16]\n",
      " [  2   4  64   4]\n",
      " [  2   8   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   8  16]\n",
      " [  2   4  64   4]\n",
      " [  2   8   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   2   8  16]\n",
      " [  0   4  64   4]\n",
      " [  4   8   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   2   8  16]\n",
      " [  0   4  64   4]\n",
      " [  4   8   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4   8  16]\n",
      " [  2   4  64   4]\n",
      " [  4   8   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4   8  16]\n",
      " [  2   4  64   4]\n",
      " [  4   8   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   8  16]\n",
      " [  2   8  64   4]\n",
      " [  4   8   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   8  16]\n",
      " [  2   8  64   4]\n",
      " [  4   8   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0   8  16]\n",
      " [  2   2  64   4]\n",
      " [  4  16   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0   8  16]\n",
      " [  2   2  64   4]\n",
      " [  4  16   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   8  16]\n",
      " [  2   4  64   4]\n",
      " [  4  16   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   8  16]\n",
      " [  2   4  64   4]\n",
      " [  4  16   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   8  16   2]\n",
      " [  2   4  64   4]\n",
      " [  4  16   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   8  16   2]\n",
      " [  2   4  64   4]\n",
      " [  4  16   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   8  16   2]\n",
      " [  4   4  64   4]\n",
      " [  4  16   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   8  16   2]\n",
      " [  4   4  64   4]\n",
      " [  4  16   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   8  16   2]\n",
      " [  2   8  64   4]\n",
      " [  4  16   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   8  16   2]\n",
      " [  2   8  64   4]\n",
      " [  4  16   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0  16   2]\n",
      " [  4  16  64   4]\n",
      " [  4  16   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 20 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0  16   2]\n",
      " [  4  16  64   4]\n",
      " [  4  16   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  4   2  16   2]\n",
      " [  4  16  64   4]\n",
      " [  4  16   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  4   2  16   2]\n",
      " [  4  16  64   4]\n",
      " [  4  16   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2  16   2]\n",
      " [  4   2  64   4]\n",
      " [  8  32   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Reward: 40 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2  16   2]\n",
      " [  4   2  64   4]\n",
      " [  8  32   4  64]\n",
      " [ 16  32  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2  16   2]\n",
      " [  4   2  64   4]\n",
      " [  8   4   4  64]\n",
      " [ 16  64  64 256]]\n",
      "Reward: 68 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2  16   2]\n",
      " [  4   2  64   4]\n",
      " [  8   4   4  64]\n",
      " [ 16  64  64 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2  16   2]\n",
      " [  4   2  64   4]\n",
      " [  2   8   8  64]\n",
      " [  0  16 128 256]]\n",
      "Reward: 136 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2  16   2]\n",
      " [  4   2  64   4]\n",
      " [  2   8   8  64]\n",
      " [  0  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2  16   2]\n",
      " [  4   2  64   4]\n",
      " [  2   2  16  64]\n",
      " [  0  16 128 256]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2  16   2]\n",
      " [  4   2  64   4]\n",
      " [  2   2  16  64]\n",
      " [  0  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2  16   2]\n",
      " [  4   2  64   4]\n",
      " [  2   4  16  64]\n",
      " [  0  16 128 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2  16   2]\n",
      " [  4   2  64   4]\n",
      " [  2   4  16  64]\n",
      " [  0  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0  16   2]\n",
      " [  0   4  64   4]\n",
      " [  4   4  16  64]\n",
      " [  2  16 128 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0  16   2]\n",
      " [  0   4  64   4]\n",
      " [  4   4  16  64]\n",
      " [  2  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2  16   2]\n",
      " [  2   4  64   4]\n",
      " [  0   8  16  64]\n",
      " [  2  16 128 256]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2  16   2]\n",
      " [  2   4  64   4]\n",
      " [  0   8  16  64]\n",
      " [  2  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2  16   2]\n",
      " [  2   4  64   4]\n",
      " [  0   8  16  64]\n",
      " [  4  16 128 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2  16   2]\n",
      " [  2   4  64   4]\n",
      " [  0   8  16  64]\n",
      " [  4  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   2  16   2]\n",
      " [  0   4  64   4]\n",
      " [  2   8  16  64]\n",
      " [  4  16 128 256]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   2  16   2]\n",
      " [  0   4  64   4]\n",
      " [  2   8  16  64]\n",
      " [  4  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4  16   2]\n",
      " [  0   4  64   4]\n",
      " [  2   8  16  64]\n",
      " [  4  16 128 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4  16   2]\n",
      " [  0   4  64   4]\n",
      " [  2   8  16  64]\n",
      " [  4  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2  16   2]\n",
      " [  0   8  64   4]\n",
      " [  4   8  16  64]\n",
      " [  4  16 128 256]]\n",
      "Reward: 12 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2  16   2]\n",
      " [  0   8  64   4]\n",
      " [  4   8  16  64]\n",
      " [  4  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0  16   2]\n",
      " [  2   2  64   4]\n",
      " [  0  16  16  64]\n",
      " [  8  16 128 256]]\n",
      "Reward: 24 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0  16   2]\n",
      " [  2   2  64   4]\n",
      " [  0  16  16  64]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   4  16   2]\n",
      " [  0   4  64   4]\n",
      " [  0   0  32  64]\n",
      " [  8  16 128 256]]\n",
      "Reward: 36 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   4  16   2]\n",
      " [  0   4  64   4]\n",
      " [  0   0  32  64]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0  16   2]\n",
      " [  0   4  64   4]\n",
      " [  0   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0  16   2]\n",
      " [  0   4  64   4]\n",
      " [  0   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[ 16   2   0   0]\n",
      " [  4  64   4   2]\n",
      " [  8  32  64   0]\n",
      " [  8  16 128 256]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[ 16   2   0   0]\n",
      " [  4  64   4   2]\n",
      " [  8  32  64   0]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0  16   2]\n",
      " [  4  64   4   2]\n",
      " [  0   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0  16   2]\n",
      " [  4  64   4   2]\n",
      " [  0   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   2  16   2]\n",
      " [  4  64   4   2]\n",
      " [  0   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   2  16   2]\n",
      " [  4  64   4   2]\n",
      " [  0   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4  16   2]\n",
      " [  4  64   4   2]\n",
      " [  0   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4  16   2]\n",
      " [  4  64   4   2]\n",
      " [  0   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  4   4  16   0]\n",
      " [  2  64   4   4]\n",
      " [  4   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  4   4  16   0]\n",
      " [  2  64   4   4]\n",
      " [  4   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2   8  16]\n",
      " [  0   2  64   8]\n",
      " [  4   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2   8  16]\n",
      " [  0   2  64   8]\n",
      " [  4   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   0   8  16]\n",
      " [  2   4  64   8]\n",
      " [  4   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   0   8  16]\n",
      " [  2   4  64   8]\n",
      " [  4   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  8  16   0   2]\n",
      " [  2   4  64   8]\n",
      " [  4   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  8  16   0   2]\n",
      " [  2   4  64   8]\n",
      " [  4   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   8  16   2]\n",
      " [  2   4  64   8]\n",
      " [  4   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   8  16   2]\n",
      " [  2   4  64   8]\n",
      " [  4   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   8  16   2]\n",
      " [  4   4  64   8]\n",
      " [  4   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   8  16   2]\n",
      " [  4   4  64   8]\n",
      " [  4   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   8  16   2]\n",
      " [  2   8  64   8]\n",
      " [  4   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   8  16   2]\n",
      " [  2   8  64   8]\n",
      " [  4   8  32  64]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0  16   2]\n",
      " [  4   8  64   8]\n",
      " [  4  16  32  64]\n",
      " [  8  16 128 256]]\n",
      "Reward: 20 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0  16   2]\n",
      " [  4   8  64   8]\n",
      " [  4  16  32  64]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   2  16   2]\n",
      " [  4   8  64   8]\n",
      " [  4  16  32  64]\n",
      " [  8  16 128 256]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   2  16   2]\n",
      " [  4   8  64   8]\n",
      " [  4  16  32  64]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4  16   2]\n",
      " [  4   8  64   8]\n",
      " [  4  16  32  64]\n",
      " [  8  16 128 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4  16   2]\n",
      " [  4   8  64   8]\n",
      " [  4  16  32  64]\n",
      " [  8  16 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   0  16   2]\n",
      " [  2   4  64   8]\n",
      " [  8   8  32  64]\n",
      " [  8  32 128 256]]\n",
      "Reward: 40 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   0  16   2]\n",
      " [  2   4  64   8]\n",
      " [  8   8  32  64]\n",
      " [  8  32 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   2  16   2]\n",
      " [  2   4  64   8]\n",
      " [  0  16  32  64]\n",
      " [  8  32 128 256]]\n",
      "Reward: 16 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   2  16   2]\n",
      " [  2   4  64   8]\n",
      " [  0  16  32  64]\n",
      " [  8  32 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   4  16   2]\n",
      " [  2   4  64   8]\n",
      " [  0  16  32  64]\n",
      " [  8  32 128 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   4  16   2]\n",
      " [  2   4  64   8]\n",
      " [  0  16  32  64]\n",
      " [  8  32 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  0   2  16   2]\n",
      " [  0   8  64   8]\n",
      " [  4  16  32  64]\n",
      " [  8  32 128 256]]\n",
      "Reward: 12 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  0   2  16   2]\n",
      " [  0   8  64   8]\n",
      " [  4  16  32  64]\n",
      " [  8  32 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2  16   2   0]\n",
      " [  8  64   8   2]\n",
      " [  4  16  32  64]\n",
      " [  8  32 128 256]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2  16   2   0]\n",
      " [  8  64   8   2]\n",
      " [  4  16  32  64]\n",
      " [  8  32 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   2  16   2]\n",
      " [  8  64   8   2]\n",
      " [  4  16  32  64]\n",
      " [  8  32 128 256]]\n",
      "Reward: 0 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   2  16   2]\n",
      " [  8  64   8   2]\n",
      " [  4  16  32  64]\n",
      " [  8  32 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  4   4  16   2]\n",
      " [  8  64   8   2]\n",
      " [  4  16  32  64]\n",
      " [  8  32 128 256]]\n",
      "Reward: 4 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  4   4  16   2]\n",
      " [  8  64   8   2]\n",
      " [  4  16  32  64]\n",
      " [  8  32 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   8  16   2]\n",
      " [  8  64   8   2]\n",
      " [  4  16  32  64]\n",
      " [  8  32 128 256]]\n",
      "Reward: 8 Done: False\n",
      "\n",
      "\n",
      "State:\n",
      "[[  2   8  16   2]\n",
      " [  8  64   8   2]\n",
      " [  4  16  32  64]\n",
      " [  8  32 128 256]]\n",
      "Q:  tensor([[-1.5262, -1.5383, -1.5175, -1.5318]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[[  2   8  16   2]\n",
      " [  8  64   8   4]\n",
      " [  4  16  32  64]\n",
      " [  8  32 128 256]]\n",
      "Reward: -100 Done: True\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_steps = 0\n",
    "for _ in range(1000):\n",
    "    print('State:')\n",
    "    print(decode_board(state))\n",
    "    Q = policy_net(state.cuda())\n",
    "    print('Q: ', Q)\n",
    "    action = Q.argmax().item()\n",
    "    state, reward, done = env.action(action)\n",
    "    while reward == -10:\n",
    "        BAMS += 1\n",
    "        Q[0, action] = float('-inf')\n",
    "        action = Q.argmax().item()\n",
    "        state, reward, done = env.action(action)\n",
    "    print(decode_board(state))\n",
    "    print('Reward:', reward, 'Done:', done)\n",
    "    print('\\n')\n",
    "    N_steps += 1\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_steps + BAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2935.0625, 2907.4143, 2930.1863, 2944.1321]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net(state.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21, 4, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.tensor(env.reset(), dtype=torch.float32, device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 2., 0.]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[554.8129, 552.2155, 553.0561, 554.3870]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (fc1): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=1024, bias=True)\n",
       "  (fc4): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "  (fc5): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (fc6): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (fc7): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (fc8): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc9): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 21])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.02856593, -0.04845341, -0.00792314,  0.02017498], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02849323,  0.04839472,  0.00306607, -0.00299951], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya/anaconda3/lib/python3.12/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/ilya/Desktop/RL/RL_2048/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/ilya/Desktop/RL/RL_2048/video/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/ilya/Desktop/RL/RL_2048/video/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ilya/Desktop/RL/RL_2048/video/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     14\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m---> 15\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/gymnasium/wrappers/record_video.py:223\u001b[0m, in \u001b[0;36mRecordVideo.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m recorded_frames \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/gymnasium/core.py:471\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/gymnasium/core.py:471\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/gymnasium/wrappers/order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m     )\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/gymnasium/wrappers/env_checker.py:67\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/gymnasium/envs/classic_control/cartpole.py:299\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m gfxdraw\u001b[38;5;241m.\u001b[39mhline(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen_width, carty, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mflip(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    301\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "#env = preprocess_env(env)  # method with some other wrappers\n",
    "env = RecordVideo(env, 'video', episode_trigger=lambda x: x == 2)\n",
    "env.start_video_recorder()\n",
    "\n",
    "for episode in range(4):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy_net(torch.tensor(state, device='cuda').unsqueeze(0)).argmax().item()\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            state = next_state\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(policy_net.state_dict(), 'policy_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1167935/4165716865.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_net.load_state_dict(torch.load('policy_net.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net.load_state_dict(torch.load('policy_net.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2., 4., 8., 0., 0., 2., 4., 0., 2., 0., 4., 0., 0., 0., 0.]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990004498800211"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.923002481460571"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.685814142227173"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
